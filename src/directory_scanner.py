#!/usr/bin/env python3
"""
Directory Scanner - é€’å½’æ‰«æç›®å½•ä¸‹çš„ç¨‹åºæ–‡ä»¶å¹¶ä½¿ç”¨ Ollama è¿›è¡Œåˆ†æ
æ”¯æŒå‡½æ•°è°ƒç”¨é“¾åˆ†æå’Œé€’å½’å®¡æ ¸
"""

import os
import sys
import re
from typing import List, Dict, Set, Optional, Pattern
import json
from datetime import datetime

# Add the src directory to the python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm.ollama_client import OllamaClient
from call_chain_analyzer import CallChainAnalyzer
from ast_analyzer import ASTAnalyzer


class DirectoryScanner:
    """æ‰«æç›®å½•å¹¶åˆ†æç¨‹åºæ–‡ä»¶çš„å·¥å…·ç±»"""
    
    # æ”¯æŒçš„ç¼–ç¨‹è¯­è¨€åŠå…¶æ–‡ä»¶æ‰©å±•å
    SUPPORTED_EXTENSIONS = {
        '.py': 'Python', '.java': 'Java', '.js': 'JavaScript', '.ts': 'TypeScript',
        '.jsx': 'React JSX', '.tsx': 'React TSX', '.cpp': 'C++', '.cc': 'C++',
        '.cxx': 'C++', '.c': 'C', '.h': 'C/C++ Header', '.hpp': 'C++ Header',
        '.cs': 'C#', '.go': 'Go', '.rs': 'Rust', '.rb': 'Ruby', '.php': 'PHP',
        '.swift': 'Swift', '.kt': 'Kotlin', '.scala': 'Scala', '.r': 'R',
        '.m': 'Objective-C', '.sh': 'Shell', '.bash': 'Bash', '.sql': 'SQL',
        '.pl': 'Perl', '.lua': 'Lua', '.dart': 'Dart', '.vue': 'Vue',
    }
    
    DEFAULT_IGNORE_DIRS = {
        '.git', '.svn', '.hg', 'node_modules', '__pycache__', '.venv', 'venv',
        'build', 'dist', 'target', 'out', '.idea', '.vscode', '.vs', 'vendor', 'packages',
    }
    
    def __init__(self, root_dir: str, output_dir: str = None, extensions: List[str] = None,
                 ignore_dirs: Set[str] = None, max_file_size: int = 1024 * 1024,
                 ollama_url: str = "http://localhost:11434", model: str = "qwen2.5:0.5b",
                 dir_pattern: Optional[str] = None, file_pattern: Optional[str] = None,
                 enable_call_chain: bool = False, enable_ast: bool = False):
        """
        åˆå§‹åŒ–ç›®å½•æ‰«æå™¨
        
        Args:
            root_dir: è¦æ‰«æçš„æ ¹ç›®å½•
            output_dir: åˆ†ææŠ¥å‘Šè¾“å‡ºç›®å½•
            extensions: è¦æ‰«æçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
            ignore_dirs: è¦å¿½ç•¥çš„ç›®å½•åç§°é›†åˆ
            max_file_size: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            ollama_url: Ollama æœåŠ¡åœ°å€
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            dir_pattern: ç›®å½•åæ­£åˆ™è¡¨è¾¾å¼ï¼ˆåŒ¹é…çš„ç›®å½•ä¼šè¢«æ‰«æï¼‰
            file_pattern: æ–‡ä»¶åæ­£åˆ™è¡¨è¾¾å¼ï¼ˆåŒ¹é…çš„æ–‡ä»¶ä¼šè¢«åˆ†æï¼‰
            enable_call_chain: æ˜¯å¦å¯ç”¨å‡½æ•°è°ƒç”¨é“¾åˆ†æ
            enable_ast: æ˜¯å¦å¯ç”¨ASTè¯­æ³•åˆ†æ
        """
        self.root_dir = os.path.abspath(root_dir)
        self.output_dir = output_dir
        self.extensions = extensions or list(self.SUPPORTED_EXTENSIONS.keys())
        self.ignore_dirs = ignore_dirs or self.DEFAULT_IGNORE_DIRS
        self.max_file_size = max_file_size
        self.ollama_url = ollama_url
        self.model = model
        self.enable_call_chain = enable_call_chain
        self.enable_ast = enable_ast
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.dir_pattern: Optional[Pattern] = re.compile(dir_pattern) if dir_pattern else None
        self.file_pattern: Optional[Pattern] = re.compile(file_pattern) if file_pattern else None
        
        if not os.path.isdir(self.root_dir):
            raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {self.root_dir}")
        
        if self.output_dir:
            os.makedirs(self.output_dir, exist_ok=True)
            print(f"âœ“ æŠ¥å‘Šå°†ä¿å­˜åˆ°: {self.output_dir}\n")
        
        # ä½¿ç”¨é…ç½®çš„ Ollama åœ°å€å’Œæ¨¡å‹
        self.ollama_client = OllamaClient(base_url=self.ollama_url, model=self.model)
        print(f"ğŸ¤– Ollama é…ç½®:")
        print(f"   æœåŠ¡åœ°å€: {self.ollama_url}")
        print(f"   æ¨¡å‹åç§°: {self.model}")
        
        if self.enable_call_chain:
            print(f"ğŸ”— è°ƒç”¨é“¾åˆ†æ: å·²å¯ç”¨")
        if self.enable_ast:
            print(f"ğŸ”¬ AST è¯­æ³•åˆ†æ: å·²å¯ç”¨")
        if not self.enable_call_chain and not self.enable_ast:
            print()
        else:
            print()
        
        self.stats = {'total_files': 0, 'analyzed_files': 0, 'skipped_files': 0, 'failed_files': 0, 'total_size': 0}
    
    def _should_scan_directory(self, dir_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰«æè¯¥ç›®å½•"""
        # å¦‚æœåœ¨å¿½ç•¥åˆ—è¡¨ä¸­ï¼Œä¸æ‰«æ
        if dir_name in self.ignore_dirs:
            return False
        
        # å¦‚æœè®¾ç½®äº†ç›®å½•æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¿…é¡»åŒ¹é…
        if self.dir_pattern:
            return self.dir_pattern.search(dir_name) is not None
        
        return True
    
    def _should_analyze_file(self, file_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ†æè¯¥æ–‡ä»¶"""
        # å¦‚æœè®¾ç½®äº†æ–‡ä»¶æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¿…é¡»åŒ¹é…
        if self.file_pattern:
            return self.file_pattern.search(file_name) is not None
        
        return True
    
    def scan_directory(self) -> List[str]:
        found_files = []
        print(f"ğŸ” å¼€å§‹æ‰«æç›®å½•: {self.root_dir}")
        print(f"ğŸ“ æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {', '.join(self.extensions)}")
        
        if self.dir_pattern:
            print(f"ğŸ“ ç›®å½•è¿‡æ»¤è§„åˆ™: {self.dir_pattern.pattern}")
        if self.file_pattern:
            print(f"ğŸ“„ æ–‡ä»¶è¿‡æ»¤è§„åˆ™: {self.file_pattern.pattern}")
        print()
        
        for root, dirs, files in os.walk(self.root_dir):
            # è¿‡æ»¤ç›®å½•
            dirs[:] = [d for d in dirs if self._should_scan_directory(d)]
            
            for file in files:
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext not in self.extensions:
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
                if not self._should_analyze_file(file):
                    continue
                
                file_path = os.path.join(root, file)
                try:
                    file_size = os.path.getsize(file_path)
                    if file_size > self.max_file_size:
                        print(f"âš ï¸  è·³è¿‡å¤§æ–‡ä»¶ ({file_size / 1024:.1f} KB): {file_path}")
                        self.stats['skipped_files'] += 1
                        continue
                    
                    self.stats['total_size'] += file_size
                    found_files.append(file_path)
                    self.stats['total_files'] += 1
                except OSError as e:
                    print(f"âš ï¸  æ— æ³•è®¿é—®æ–‡ä»¶: {file_path} - {e}")
        
        print(f"\nâœ“ æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(found_files)} ä¸ªæ–‡ä»¶")
        print(f"  æ€»å¤§å°: {self.stats['total_size'] / 1024:.2f} KB\n")
        return found_files
    
    def get_analysis_prompt(self, file_path: str, content: str, language: str) -> str:
        return f"""è¯·åˆ†æä»¥ä¸‹ {language} ä»£ç æ–‡ä»¶å¹¶æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚

æ–‡ä»¶è·¯å¾„: {file_path}
ç¼–ç¨‹è¯­è¨€: {language}

ä»£ç å†…å®¹:
```{language.lower()}
{content}
```

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æï¼š

1. **ä»£ç æ¦‚è¿°** - æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½å’Œç”¨é€”ï¼Œæ ¸å¿ƒç±»ã€å‡½æ•°æˆ–æ¨¡å—çš„è¯´æ˜
2. **ä»£ç è´¨é‡** - ä»£ç ç»“æ„å’Œç»„ç»‡ã€å‘½åè§„èŒƒã€æ³¨é‡Šå®Œæ•´æ€§ã€ä»£ç å¤æ‚åº¦è¯„ä¼°
3. **æ½œåœ¨é—®é¢˜** - å¯èƒ½çš„ bug æˆ–é€»è¾‘é”™è¯¯ã€æ€§èƒ½é—®é¢˜ã€å®‰å…¨éšæ‚£ã€ä»£ç å¼‚å‘³
4. **æ”¹è¿›å»ºè®®** - é‡æ„å»ºè®®ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®ã€æœ€ä½³å®è·µå»ºè®®ã€å¯ç»´æŠ¤æ€§æ”¹è¿›
5. **ä¾èµ–å…³ç³»** - å¯¼å…¥çš„åº“å’Œæ¨¡å—ã€å¤–éƒ¨ä¾èµ–

è¯·ä»¥ Markdown æ ¼å¼è¾“å‡ºåˆ†ææŠ¥å‘Šï¼Œä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œåˆ—è¡¨ã€‚"""
    def analyze_file(self, file_path: str) -> Dict:
        rel_path = os.path.relpath(file_path, self.root_dir)
        file_ext = os.path.splitext(file_path)[1].lower()
        language = self.SUPPORTED_EXTENSIONS.get(file_ext, 'Unknown')
        
        result = {
            'file_path': rel_path, 
            'language': language, 
            'status': 'pending', 
            'analysis': None, 
            'error': None,
            'call_chain': None
        }
        
        print(f"{'='*80}")
        print(f"ğŸ“„ åˆ†ææ–‡ä»¶: {rel_path}")
        print(f"ğŸ”¤ è¯­è¨€: {language}")
        print(f"{'='*80}\n")
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # è°ƒç”¨é“¾åˆ†æ
            call_chain_info = None
            if self.enable_call_chain:
                print("ğŸ”— æ­£åœ¨åˆ†æå‡½æ•°è°ƒç”¨é“¾...")
                call_chain_info = self._analyze_call_chain(content, file_path, language)
                result['call_chain'] = call_chain_info
                print(f"âœ“ å‘ç° {len(call_chain_info.get('functions', []))} ä¸ªå‡½æ•°\n")
            
            # AST è¯­æ³•åˆ†æ
            ast_info = None
            if self.enable_ast:
                print("ğŸ”¬ æ­£åœ¨è¿›è¡Œ AST è¯­æ³•åˆ†æ...")
                ast_info = self._analyze_ast(content, file_path, language)
                result['ast_analysis'] = ast_info
                if ast_info:
                    print(f"âœ“ æå–äº† {len(ast_info.get('classes', []))} ä¸ªç±», {len(ast_info.get('functions', []))} ä¸ªå‡½æ•°\n")
            
            # åŸºç¡€ä»£ç åˆ†æ
            prompt = self.get_analysis_prompt(rel_path, content, language, call_chain_info, ast_info)
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨ Ollama è¿›è¡Œåˆ†æ...")
            analysis = self.ollama_client.generate_response(prompt)
            
            result['status'] = 'success'
            result['analysis'] = analysis
            self.stats['analyzed_files'] += 1
            
            print("\n" + "="*80)
            print("ğŸ“Š åˆ†æç»“æœ")
            print("="*80)
            print(analysis)
            print("\n")
            
            if self.output_dir:
                self._save_analysis(rel_path, language, analysis, call_chain_info, ast_info)
            
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            self.stats['failed_files'] += 1
            print(f"âŒ åˆ†æå¤±è´¥: {e}\n")
        
        return result
    
    def _analyze_call_chain(self, content: str, file_path: str, language: str) -> Dict:
        """
        åˆ†æå‡½æ•°è°ƒç”¨é“¾
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            è°ƒç”¨é“¾ä¿¡æ¯å­—å…¸
        """
        analyzer = CallChainAnalyzer(language=language)
        call_graph = analyzer.build_call_graph(content, file_path)
        
        # ç”Ÿæˆè°ƒç”¨é“¾æŠ¥å‘Š
        call_chain_report = analyzer.generate_call_chain_report()
        mermaid_diagram = analyzer.generate_mermaid_diagram()
        
        return {
            'functions': call_graph['functions'],
            'call_graph': call_graph['call_graph'],
            'reverse_call_graph': call_graph['reverse_call_graph'],
            'report': call_chain_report,
            'mermaid': mermaid_diagram
        }
    
    def _analyze_ast(self, content: str, file_path: str, language: str) -> Optional[Dict]:
        """
        è¿›è¡Œ AST è¯­æ³•åˆ†æ
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_path: æ–‡ä»¶è·¯å¾„
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            AST åˆ†æä¿¡æ¯å­—å…¸
        """
        try:
            analyzer = ASTAnalyzer(language=language)
            ast_result = analyzer.analyze_file(file_path)
            
            # æ„å»ºä¾èµ–å›¾ï¼ˆå¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶å¯ä»¥ä¼ å…¥ï¼‰
            # dependency_graph = analyzer.build_dependency_graph([file_path])
            
            return {
                'classes': ast_result.get('classes', []),
                'functions': ast_result.get('functions', []),
                'imports': ast_result.get('imports', []),
                'calls': ast_result.get('calls', []),
                'package': ast_result.get('package'),
                'interfaces': ast_result.get('interfaces', [])
            }
        except Exception as e:
            print(f"âš ï¸  AST åˆ†æå¤±è´¥: {e}")
            return None
    
    def get_analysis_prompt(self, file_path: str, content: str, language: str, 
                           call_chain_info: Optional[Dict] = None,
                           ast_info: Optional[Dict] = None) -> str:
        """ç”Ÿæˆåˆ†ææç¤ºè¯ï¼ŒåŒ…å«è°ƒç”¨é“¾ä¿¡æ¯å’ŒASTä¿¡æ¯"""
        
        base_prompt = f"""è¯·åˆ†æä»¥ä¸‹ {language} ä»£ç æ–‡ä»¶å¹¶æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚

æ–‡ä»¶è·¯å¾„: {file_path}
ç¼–ç¨‹è¯­è¨€: {language}

ä»£ç å†…å®¹:
```{language.lower()}
{content}
```
"""
        
        # å¦‚æœæœ‰ASTåˆ†æä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
        if ast_info:
            base_prompt += f"""

## AST è¯­æ³•ç»“æ„åˆ†æ

"""
            if ast_info.get('package'):
                base_prompt += f"**åŒ…å**: `{ast_info['package']}`\n\n"
            
            if ast_info.get('classes'):
                base_prompt += f"**ç±»å®šä¹‰** ({len(ast_info['classes'])} ä¸ª):\n"
                for cls in ast_info['classes'][:10]:
                    base_prompt += f"- `{cls['name']}`"
                    if cls.get('parent'):
                        base_prompt += f" extends `{cls['parent']}`"
                    if cls.get('interfaces'):
                        base_prompt += f" implements `{', '.join(cls['interfaces'])}`"
                    base_prompt += f" (ç¬¬ {cls.get('line', 'N/A')} è¡Œ)\n"
                base_prompt += "\n"
            
            if ast_info.get('functions'):
                base_prompt += f"**å‡½æ•°å®šä¹‰** ({len(ast_info['functions'])} ä¸ª):\n"
                for func in ast_info['functions'][:10]:
                    base_prompt += f"- `{func['name']}` (ç¬¬ {func.get('line', 'N/A')} è¡Œ)\n"
                base_prompt += "\n"
            
            if ast_info.get('imports'):
                base_prompt += f"**å¯¼å…¥ä¾èµ–** ({len(ast_info['imports'])} ä¸ª):\n"
                for imp in ast_info['imports'][:15]:
                    base_prompt += f"- `{imp}`\n"
                base_prompt += "\n"
        
        # å¦‚æœæœ‰è°ƒç”¨é“¾ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
        if call_chain_info and call_chain_info.get('functions'):
            base_prompt += f"""

## å‡½æ•°è°ƒç”¨é“¾ä¿¡æ¯

è¯¥æ–‡ä»¶åŒ…å« {len(call_chain_info['functions'])} ä¸ªå‡½æ•°ï¼š
"""
            for func in call_chain_info['functions'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                base_prompt += f"- `{func['signature']}` (ç¬¬ {func['start_line']}-{func['end_line']} è¡Œ)\n"
            
            if call_chain_info.get('call_graph'):
                base_prompt += "\nè°ƒç”¨å…³ç³»:\n"
                for caller, callees in list(call_chain_info['call_graph'].items())[:5]:
                    base_prompt += f"- `{caller}` è°ƒç”¨: {', '.join(f'`{c}`' for c in callees)}\n"
        
        base_prompt += """

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æï¼š

1. **ä»£ç æ¦‚è¿°** - æ–‡ä»¶çš„ä¸»è¦åŠŸèƒ½å’Œç”¨é€”ï¼Œæ ¸å¿ƒç±»ã€å‡½æ•°æˆ–æ¨¡å—çš„è¯´æ˜
2. **ä»£ç è´¨é‡** - ä»£ç ç»“æ„å’Œç»„ç»‡ã€å‘½åè§„èŒƒã€æ³¨é‡Šå®Œæ•´æ€§ã€ä»£ç å¤æ‚åº¦è¯„ä¼°
3. **æ½œåœ¨é—®é¢˜** - å¯èƒ½çš„ bug æˆ–é€»è¾‘é”™è¯¯ã€æ€§èƒ½é—®é¢˜ã€å®‰å…¨éšæ‚£ã€ä»£ç å¼‚å‘³
4. **æ”¹è¿›å»ºè®®** - é‡æ„å»ºè®®ã€æ€§èƒ½ä¼˜åŒ–å»ºè®®ã€æœ€ä½³å®è·µå»ºè®®ã€å¯ç»´æŠ¤æ€§æ”¹è¿›
5. **ä¾èµ–å…³ç³»** - å¯¼å…¥çš„åº“å’Œæ¨¡å—ã€å¤–éƒ¨ä¾èµ–
"""
        
        if ast_info:
            base_prompt += """6. **AST ç»“æ„åˆ†æ** - åŸºäºä¸Šè¿° AST ä¿¡æ¯ï¼Œåˆ†æï¼š
   - ç±»çš„è®¾è®¡å’ŒèŒè´£åˆ’åˆ†
   - ç»§æ‰¿å’Œæ¥å£å®ç°çš„åˆç†æ€§
   - ä¾èµ–æ³¨å…¥å’Œè§£è€¦ç¨‹åº¦
   - æ¨¡å—åŒ–ç¨‹åº¦
"""
        
        if call_chain_info:
            base_prompt += """7. **å‡½æ•°è°ƒç”¨é“¾åˆ†æ** - åŸºäºä¸Šè¿°è°ƒç”¨é“¾ä¿¡æ¯ï¼Œåˆ†æï¼š
   - å…³é”®å‡½æ•°çš„è°ƒç”¨è·¯å¾„
   - å¯èƒ½çš„å¾ªç¯è°ƒç”¨æˆ–æ·±åº¦è°ƒç”¨é—®é¢˜
   - å‡½æ•°èŒè´£æ˜¯å¦å•ä¸€
   - è°ƒç”¨å±‚æ¬¡æ˜¯å¦åˆç†
"""
        
        base_prompt += "\nè¯·ä»¥ Markdown æ ¼å¼è¾“å‡ºåˆ†ææŠ¥å‘Šï¼Œä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œåˆ—è¡¨ã€‚"
        
        return base_prompt
    
    def _save_analysis(self, file_path: str, language: str, analysis: str, 
                      call_chain_info: Optional[Dict] = None, ast_info: Optional[Dict] = None):
        safe_path = file_path.replace(os.sep, '_').replace('.', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = os.path.join(self.output_dir, f"{safe_path}_analysis_{timestamp}.md")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# ä»£ç åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**æ–‡ä»¶è·¯å¾„**: `{file_path}`\n\n")
            f.write(f"**ç¼–ç¨‹è¯­è¨€**: {language}\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if call_chain_info:
                f.write(f"**å‡½æ•°æ•°é‡**: {len(call_chain_info.get('functions', []))}\n\n")
            if ast_info:
                f.write(f"**ç±»æ•°é‡**: {len(ast_info.get('classes', []))}\n\n")
                f.write(f"**å¯¼å…¥æ•°é‡**: {len(ast_info.get('imports', []))}\n\n")
            
            f.write("---\n\n")
            
            # å¦‚æœæœ‰ASTä¿¡æ¯ï¼Œå…ˆå†™å…¥ASTåˆ†æ
            if ast_info:
                f.write("## ğŸ”¬ AST è¯­æ³•ç»“æ„åˆ†æ\n\n")
                
                if ast_info.get('package'):
                    f.write(f"**åŒ…å**: `{ast_info['package']}`\n\n")
                
                if ast_info.get('classes'):
                    f.write(f"### ç±»å®šä¹‰ ({len(ast_info['classes'])} ä¸ª)\n\n")
                    for cls in ast_info['classes']:
                        f.write(f"#### `{cls['name']}`\n")
                        if cls.get('parent'):
                            f.write(f"- ç»§æ‰¿: `{cls['parent']}`\n")
                        if cls.get('interfaces'):
                            f.write(f"- å®ç°æ¥å£: `{', '.join(cls['interfaces'])}`\n")
                        if cls.get('methods'):
                            f.write(f"- æ–¹æ³•æ•°: {len(cls['methods'])}\n")
                        f.write("\n")
                
                if ast_info.get('imports'):
                    f.write(f"### å¯¼å…¥ä¾èµ– ({len(ast_info['imports'])} ä¸ª)\n\n")
                    for imp in ast_info['imports']:
                        f.write(f"- `{imp}`\n")
                    f.write("\n")
                
                f.write("---\n\n")
            
            # å¦‚æœæœ‰è°ƒç”¨é“¾ä¿¡æ¯ï¼Œå†™å…¥è°ƒç”¨é“¾æŠ¥å‘Š
            if call_chain_info:
                f.write("## ğŸ“Š å‡½æ•°è°ƒç”¨é“¾åˆ†æ\n\n")
                f.write(call_chain_info.get('report', ''))
                f.write("\n\n### è°ƒç”¨å…³ç³»å›¾\n\n")
                f.write(call_chain_info.get('mermaid', ''))
                f.write("\n\n---\n\n")
            
            # å†™å…¥ä»£ç åˆ†æç»“æœ
            f.write("## ğŸ¤– AI ä»£ç åˆ†æ\n\n")
            f.write(analysis)
        
        print(f"âœ“ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}\n")
        
        # å¦‚æœæœ‰è°ƒç”¨é“¾ä¿¡æ¯ï¼ŒåŒæ—¶ä¿å­˜JSONæ ¼å¼
        if call_chain_info:
            json_file = os.path.join(self.output_dir, f"{safe_path}_callchain_{timestamp}.json")
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'file_path': file_path,
                    'language': language,
                    'timestamp': datetime.now().isoformat(),
                    'functions': call_chain_info.get('functions', []),
                    'call_graph': call_chain_info.get('call_graph', {}),
                    'reverse_call_graph': call_chain_info.get('reverse_call_graph', {})
                }, f, ensure_ascii=False, indent=2)
            print(f"âœ“ è°ƒç”¨é“¾æ•°æ®å·²ä¿å­˜: {json_file}\n")
        
        # å¦‚æœæœ‰ASTä¿¡æ¯ï¼Œä¿å­˜JSONæ ¼å¼
        if ast_info:
            ast_json_file = os.path.join(self.output_dir, f"{safe_path}_ast_{timestamp}.json")
            with open(ast_json_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'file_path': file_path,
                    'language': language,
                    'timestamp': datetime.now().isoformat(),
                    'ast_analysis': ast_info
                }, f, ensure_ascii=False, indent=2)
            print(f"âœ“ AST æ•°æ®å·²ä¿å­˜: {ast_json_file}\n")
    
    def analyze_all(self) -> List[Dict]:
        files = self.scan_directory()
        
        if not files:
            print("âš ï¸  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return []
        
        results = []
        for i, file_path in enumerate(files, 1):
            print(f"\nè¿›åº¦: [{i}/{len(files)}]")
            result = self.analyze_file(file_path)
            results.append(result)
        
        self._print_summary()
        
        if self.output_dir:
            self._save_summary(results)
        
        return results
    
    def _print_summary(self):
        print("\n" + "="*80)
        print("ğŸ“ˆ åˆ†æç»Ÿè®¡")
        print("="*80)
        print(f"æ‰«æçš„æ–‡ä»¶æ€»æ•°: {self.stats['total_files']}")
        print(f"æˆåŠŸåˆ†æ: {self.stats['analyzed_files']}")
        print(f"è·³è¿‡çš„æ–‡ä»¶: {self.stats['skipped_files']}")
        print(f"å¤±è´¥çš„æ–‡ä»¶: {self.stats['failed_files']}")
        print(f"æ€»æ–‡ä»¶å¤§å°: {self.stats['total_size'] / 1024:.2f} KB")
        print("="*80)
    
    def _save_summary(self, results: List[Dict]):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_file = os.path.join(self.output_dir, f"summary_{timestamp}.md")
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"# ä»£ç åˆ†ææ±‡æ€»æŠ¥å‘Š\n\n")
            f.write(f"**æ‰«æç›®å½•**: `{self.root_dir}`\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("## ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- æ‰«æçš„æ–‡ä»¶æ€»æ•°: {self.stats['total_files']}\n")
            f.write(f"- æˆåŠŸåˆ†æ: {self.stats['analyzed_files']}\n")
            f.write(f"- è·³è¿‡çš„æ–‡ä»¶: {self.stats['skipped_files']}\n")
            f.write(f"- å¤±è´¥çš„æ–‡ä»¶: {self.stats['failed_files']}\n")
            f.write(f"- æ€»æ–‡ä»¶å¤§å°: {self.stats['total_size'] / 1024:.2f} KB\n\n")
            f.write("## åˆ†æç»“æœ\n\n")
            for result in results:
                status_emoji = "âœ…" if result['status'] == 'success' else "âŒ"
                f.write(f"{status_emoji} **{result['file_path']}** ({result['language']})\n")
                if result['error']:
                    f.write(f"   - é”™è¯¯: {result['error']}\n")
                f.write("\n")
        
        print(f"\nâœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
        
        json_file = os.path.join(self.output_dir, f"summary_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'root_dir': self.root_dir,
                'timestamp': datetime.now().isoformat(),
                'stats': self.stats,
                'results': results,
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ JSON æŠ¥å‘Šå·²ä¿å­˜: {json_file}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='é€’å½’æ‰«æç›®å½•ä¸‹çš„ç¨‹åºæ–‡ä»¶å¹¶ä½¿ç”¨ Ollama è¿›è¡Œåˆ†æ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python directory_scanner.py /path/to/project -o reports
  
  # ä½¿ç”¨è¿œç¨‹ Ollama æœåŠ¡
  python directory_scanner.py /path/to/project --ollama-url http://192.168.1.100:11434
  
  # ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
  python directory_scanner.py /path/to/project --model qwen2.5:7b
  
  # åªåˆ†æç‰¹å®šæ‰©å±•åçš„æ–‡ä»¶
  python directory_scanner.py /path/to/project -e .py .java
  
  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤æ–‡ä»¶ï¼ˆåªåˆ†æåŒ…å« "test" çš„æ–‡ä»¶ï¼‰
  python directory_scanner.py /path/to/project --file-pattern ".*test.*"
  
  # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤ç›®å½•ï¼ˆåªæ‰«æ src å’Œ lib ç›®å½•ï¼‰
  python directory_scanner.py /path/to/project --dir-pattern "^(src|lib)$"
  
  # ç»„åˆä½¿ç”¨
  python directory_scanner.py /path/to/project \\
    --ollama-url http://192.168.1.100:11434 \\
    --model qwen2.5:7b \\
    -e .py .java \\
    --file-pattern ".*Service.*" \\
    -o reports
        """
    )
    
    parser.add_argument('directory', help='è¦æ‰«æçš„ç›®å½•è·¯å¾„')
    parser.add_argument('-o', '--output', dest='output_dir', help='åˆ†ææŠ¥å‘Šè¾“å‡ºç›®å½•')
    parser.add_argument('-e', '--extensions', nargs='+', help='è¦æ‰«æçš„æ–‡ä»¶æ‰©å±•åï¼ˆä¾‹å¦‚: .py .java .jsï¼‰')
    parser.add_argument('--max-size', type=int, default=1024 * 1024, help='æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤ 1MB')
    parser.add_argument('--ignore-dirs', nargs='+', help='è¦å¿½ç•¥çš„ç›®å½•åç§°')
    
    # Ollama é…ç½®å‚æ•°
    parser.add_argument('--ollama-url', default='http://localhost:11434', 
                       help='Ollama æœåŠ¡åœ°å€ï¼ˆé»˜è®¤: http://localhost:11434ï¼‰')
    parser.add_argument('--model', default='qwen2.5:0.5b',
                       help='ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆé»˜è®¤: qwen2.5:0.5bï¼‰')
    
    # æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤å‚æ•°
    parser.add_argument('--dir-pattern', help='ç›®å½•åæ­£åˆ™è¡¨è¾¾å¼ï¼ˆåªæ‰«æåŒ¹é…çš„ç›®å½•ï¼‰')
    parser.add_argument('--file-pattern', help='æ–‡ä»¶åæ­£åˆ™è¡¨è¾¾å¼ï¼ˆåªåˆ†æåŒ¹é…çš„æ–‡ä»¶ï¼‰')
    
    # é«˜çº§åˆ†æå‚æ•°
    parser.add_argument('--enable-call-chain', action='store_true',
                       help='å¯ç”¨å‡½æ•°è°ƒç”¨é“¾åˆ†æï¼ˆç”Ÿæˆè°ƒç”¨å›¾å’Œé€’å½’å®¡æ ¸ï¼‰')
    parser.add_argument('--enable-ast', action='store_true',
                       help='å¯ç”¨ASTè¯­æ³•åˆ†æï¼ˆæå–ç±»ã€æ–¹æ³•ã€ä¾èµ–å…³ç³»ï¼‰')
    
    args = parser.parse_args()
    
    try:
        scanner = DirectoryScanner(
            root_dir=args.directory,
            output_dir=args.output_dir,
            extensions=args.extensions,
            ignore_dirs=set(args.ignore_dirs) if args.ignore_dirs else None,
            max_file_size=args.max_size,
            ollama_url=args.ollama_url,
            model=args.model,
            dir_pattern=args.dir_pattern,
            file_pattern=args.file_pattern,
            enable_call_chain=args.enable_call_chain,
            enable_ast=args.enable_ast
        )
        scanner.analyze_all()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

